{
  "cells": [
    {
      "cell_type": "code",
      "id": "dGOYfdKMxoondAjz9lEvXAFl",
      "metadata": {
        "tags": [],
        "id": "dGOYfdKMxoondAjz9lEvXAFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dacc1d0-d717-43fb-9f6f-1b97de8f77de"
      },
      "source": [
        "!pip install --upgrade --force-reinstall google-cloud-bigquery google-cloud-bigquery-storage google-cloud-aiplatform pyarrow pandas google-cloud-aiplatform langchain-google-genai langchain-google-vertexai langchain-google-community langchain-core --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.43.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.17.0 requires google-cloud-storage<3.0.0,>=2.18.0, but you have google-cloud-storage 3.6.0 which is incompatible.\n",
            "google-adk 1.17.0 requires tenacity<9.0.0,>=8.0.0, but you have tenacity 9.1.2 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.1.0 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\n",
            "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "izYYLNAwlPm4"
      },
      "id": "izYYLNAwlPm4"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud import aiplatform\n",
        "from vertexai.preview.language_models import TextEmbeddingModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "qgeDbYaflRg2"
      },
      "id": "qgeDbYaflRg2",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create table from csv"
      ],
      "metadata": {
        "id": "z6cjQSeCljDn"
      },
      "id": "z6cjQSeCljDn"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PROJECT_ID = \"qwiklabs-gcp-02-9a09090caa33\"\n",
        "DATASET = \"aurora_bay\"\n",
        "TABLE = \"faqs\"\n",
        "\n",
        "bq = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Create dataset if not exists\n",
        "dataset_ref = bigquery.Dataset(f\"{PROJECT_ID}.{DATASET}\")\n",
        "dataset_ref.location = \"US\"\n",
        "bq.create_dataset(dataset_ref, exists_ok=True)\n",
        "\n",
        "df = pd.read_csv(\"aurora-bay-faqs.csv\")\n",
        "\n",
        "# Upload to BigQuery\n",
        "bq.load_table_from_dataframe(df, f\"{PROJECT_ID}.{DATASET}.{TABLE}\").result()\n",
        "\n",
        "print(\"Loaded\", len(df), \"rows into BigQuery table.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUxJ8RUZlY78",
        "outputId": "ac698fb9-f1f6-4a51-a3a0-3b024346bb2e"
      },
      "id": "fUxJ8RUZlY78",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 50 rows into BigQuery table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create BQ connection"
      ],
      "metadata": {
        "id": "5CB9CildmII7"
      },
      "id": "5CB9CildmII7"
    },
    {
      "cell_type": "code",
      "source": [
        "!bq mk --connection --location=US --connection_type=CLOUD_RESOURCE embedding_conn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sTe804wmHZP",
        "outputId": "1540938d-279b-44db-df51-f916d10a63ec"
      },
      "id": "3sTe804wmHZP",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery error in mk operation: Already Exists: Connection\n",
            "projects/666996842470/locations/us/connections/embedding_conn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create embeddings model"
      ],
      "metadata": {
        "id": "0By3tnisoOfC"
      },
      "id": "0By3tnisoOfC"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model_sql = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{DATASET}.embeddings`\n",
        "REMOTE WITH CONNECTION `us.embedding_conn`\n",
        "OPTIONS (ENDPOINT = 'text-embedding-005');\n",
        "\"\"\"\n",
        "\n",
        "job = bq.query(embedding_model_sql)\n",
        "job.result()\n",
        "print(\"Remote embedding model created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MKdMUJ-oOA0",
        "outputId": "62597d87-c80d-45f3-a3e4-147c22008a71"
      },
      "id": "4MKdMUJ-oOA0",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remote embedding model created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create embeddings table"
      ],
      "metadata": {
        "id": "BKtC1zw6pbVI"
      },
      "id": "BKtC1zw6pbVI"
    },
    {
      "cell_type": "code",
      "source": [
        "generate_embeddings_sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "`{DATASET}.faqs_with_embedding` AS\n",
        "SELECT *\n",
        "FROM ML.GENERATE_EMBEDDING(\n",
        " MODEL `{DATASET}.embeddings`,\n",
        " (SELECT CONCAT(question, ' ', answer) AS content, question, answer FROM\n",
        "`{DATASET}.faqs`));\n",
        "\"\"\"\n",
        "job = bq.query(generate_embeddings_sql)\n",
        "job.result()\n",
        "print(\"Embedding table created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qieglNkGpdFt",
        "outputId": "79cfb960-347a-4a5e-f9cf-ebbf52b125a9"
      },
      "id": "qieglNkGpdFt",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding table created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using langchain create an embedding class and a vector store"
      ],
      "metadata": {
        "id": "-miLur2iqc4h"
      },
      "id": "-miLur2iqc4h"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "from langchain_google_community import BigQueryVectorStore\n",
        "\n",
        "embedding = VertexAIEmbeddings(\n",
        "    model_name=\"text-embedding-005\", project=PROJECT_ID\n",
        ")\n",
        "\n",
        "store = BigQueryVectorStore(\n",
        "    project_id=PROJECT_ID,\n",
        "    dataset_name=DATASET,\n",
        "    table_name=\"faqs_with_embedding\",\n",
        "    location=\"US\",\n",
        "    embedding=embedding,\n",
        "    embedding_field=\"ml_generate_embedding_result\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1DpN0l71mIN",
        "outputId": "84f8452c-2c9f-46f8-998f-57babeba0055"
      },
      "id": "t1DpN0l71mIN",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_google_community.bq_storage_vectorstores._base:BigQuery table qwiklabs-gcp-02-9a09090caa33.aurora_bay.faqs_with_embedding initialized/validated as persistent storage. Access via BigQuery console:\n",
            " https://console.cloud.google.com/bigquery?project=qwiklabs-gcp-02-9a09090caa33&ws=!1m5!1m4!4m3!1sqwiklabs-gcp-02-9a09090caa33!2saurora_bay!3sfaqs_with_embedding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a chain"
      ],
      "metadata": {
        "id": "1OlVSJ5h6lBW"
      },
      "id": "1OlVSJ5h6lBW"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_google_vertexai import VertexAI\n",
        "\n",
        "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
        "prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
        "llm = VertexAI(model_name=\"gemini-2.0-flash\")\n",
        "\n",
        "qa_chain = (\n",
        "    {\n",
        "        \"context\": store.as_retriever(),\n",
        "        \"input\": RunnablePassthrough(),\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "xhlRgkvG5Kez"
      },
      "id": "xhlRgkvG5Kez",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain.invoke(\"what is the capitol of Alaska?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E2p-eu5H5LIQ",
        "outputId": "eb316cd8-4189-4df1-b87a-d3e8ed451b6f"
      },
      "id": "E2p-eu5H5LIQ",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the context provided, the answer to your question is not available.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat loop"
      ],
      "metadata": {
        "id": "-UHPPBqf6avR"
      },
      "id": "-UHPPBqf6avR"
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_input = input(\"You: \")\n",
        "  if user_input.lower().strip() in ['exit', 'quit']:\n",
        "        print(\"Chat session ended\")\n",
        "        break\n",
        "  print(\"Bot:\")\n",
        "  print(qa_chain.invoke(user_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXwag8GM6Jrw",
        "outputId": "77494df7-9d87-4349-ad5a-4ed476226101"
      },
      "id": "vXwag8GM6Jrw",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Where can I go to watch whales\n",
            "Bot:\n",
            "The Aurora Bay Harbor area offers whale watching tours, especially during spring and summer when humpbacks and orcas frequent the region.\n",
            "You: Is there public transity\n",
            "Bot:\n",
            "Yes. Aurora Bay operates a limited bus service on weekdays from 6 AM to 8 PM, servicing main routes including downtown, the airport, and residential neighborhoods.\n",
            "You: exit\n",
            "Chat session ended\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "challenge-2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}